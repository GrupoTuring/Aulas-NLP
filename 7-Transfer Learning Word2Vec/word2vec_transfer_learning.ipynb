{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfkHdHvsgtXa"
      },
      "source": [
        "## Dataset \n",
        "\n",
        "Esse dataset contém 5 colunas:\n",
        "* **ID:** ID do tweet\n",
        "* **Keyword**: Uma palavra chave do tweet (pode estar em branco)\n",
        "* **Location:** A localização de onde o tweet foi postado(pode estar em branco)\n",
        "* **Text:** coluna onde estão os tweets\n",
        "* **Target:** mostra se o tweet fala de uma **desastre real (1)** ou **falso (0)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWv59n8EjtvT"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzXhEfORqEGZ"
      },
      "source": [
        "Vamos dar uma olhada no nosso Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnDVLeSEZZct",
        "outputId": "9817c53e-0a74-439d-ba4d-1a8aa13dc63c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-l6-ky-nAhW",
        "outputId": "b2fcf683-d50f-4ab3-b04c-a0319b5f41f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = df_bert = pd.read_csv('/content/drive/Shared drives/Grupo Turing/P&D/Áreas de Foco/NLP/Materiais/Aulas 2020/Aula Transfer Learning Word2Vec/datasets/train.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AJrXR-Y1VFl"
      },
      "source": [
        "## Pré-processamento\n",
        "\n",
        "Antes de continuarmos a análise vamos pré-processar nossos tweets, peguei parte do código do pré-processamento que a Camilala, a Lu e a Ju fizeram para a aulinha de EDA dos tweets do corona, obrigada meninas <3 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woIdxcKfO9D-",
        "outputId": "7d28baf1-b39e-4f51-9f6c-e8e55bdcc050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL60sfpHPItD"
      },
      "source": [
        "spc = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUN1GGXYnk03"
      },
      "source": [
        "df = df.drop_duplicates('id', keep=\"first\")\n",
        "df.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfBrOQp6XfRh",
        "outputId": "c4356529-98af-4b58-92a6-f26f169513be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>48</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Birmingham</td>\n",
              "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>49</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Est. September 2012 - Bristol</td>\n",
              "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>50</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>AFRICA</td>\n",
              "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>52</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Philadelphia, PA</td>\n",
              "      <td>Crying out for more! Set me ablaze</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>53</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>London, UK</td>\n",
              "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id keyword  ...                                               text target\n",
              "31  48  ablaze  ...  @bbcmtd Wholesale Markets ablaze http://t.co/l...      1\n",
              "32  49  ablaze  ...  We always try to bring the heavy. #metal #RT h...      0\n",
              "33  50  ablaze  ...  #AFRICANBAZE: Breaking news:Nigeria flag set a...      1\n",
              "34  52  ablaze  ...                 Crying out for more! Set me ablaze      0\n",
              "35  53  ablaze  ...  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B--6xgOPQ9b"
      },
      "source": [
        "def limpa_tweets(texto):\n",
        "   # Remove links e '<user>' e converte para letra minuscula\n",
        "    sem_links = re.sub(r\"http\\S+\", \"\", texto.lower())\n",
        "    sem_user = re.sub(\"<user>\", \"\", sem_links)\n",
        "\n",
        "    # Remove caracteres que nao sao letras e tokeniza\n",
        "    tokens =  re.findall(r'\\b[A-zÀ-úü]+\\b', sem_user)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    stopw = nltk.corpus.stopwords.words('english')\n",
        "    stopw_set = set(stopw)\n",
        "\n",
        "    sem_stopwords = [w for w in tokens if w not in stopw_set]\n",
        "\n",
        "    junta_texto = \" \".join(sem_stopwords)\n",
        "\n",
        "    # Instanciando o objeto spacy\n",
        "    spc_letras =  spc(junta_texto)\n",
        "\n",
        "    # Lemmização \n",
        "    lemma = [token.lemma_ if token.pos_ == 'VERB' else str(token) for token in spc_letras]\n",
        "  \n",
        "    # juntando os tokens \n",
        "    tweets_processados = \" \".join(lemma)\n",
        "  \n",
        "    return sem_stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujUgGJl3RQzh"
      },
      "source": [
        "df['text'] = df['text'].apply(limpa_tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjM7zMRJW3Bs",
        "outputId": "a4d945b0-58b2-4e21-dcbf-682eaac24153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>48</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Birmingham</td>\n",
              "      <td>[bbcmtd, wholesale, markets, ablaze]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>49</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Est. September 2012 - Bristol</td>\n",
              "      <td>[always, try, bring, heavy, metal, rt]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>50</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>AFRICA</td>\n",
              "      <td>[africanbaze, breaking, news, nigeria, flag, s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>52</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Philadelphia, PA</td>\n",
              "      <td>[crying, set, ablaze]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>53</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>London, UK</td>\n",
              "      <td>[plus, side, look, sky, last, night, ablaze]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id keyword  ...                                               text target\n",
              "31  48  ablaze  ...               [bbcmtd, wholesale, markets, ablaze]      1\n",
              "32  49  ablaze  ...             [always, try, bring, heavy, metal, rt]      0\n",
              "33  50  ablaze  ...  [africanbaze, breaking, news, nigeria, flag, s...      1\n",
              "34  52  ablaze  ...                              [crying, set, ablaze]      0\n",
              "35  53  ablaze  ...       [plus, side, look, sky, last, night, ablaze]      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5jgRqNzaqeX"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6_hd_bhcwkg"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoiNZtOxRmsU"
      },
      "source": [
        "### Layer de Embedding do Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzcQk2i5RsLh"
      },
      "source": [
        "vocab_len = 10\n",
        "embedding_dim = 5\n",
        "embedding_matrix = np.random.randn(vocab_len, embedding_dim)\n",
        "\n",
        "embedding_example = Embedding(vocab_len, embedding_dim, \n",
        "                              embeddings_initializer = Constant(embedding_matrix),\n",
        "                              trainable = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMlVKuHmYrAT",
        "outputId": "2eda9af9-a272-4228-d924-429d74b1437f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# One hot equivalente = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "#               indices: 0  1  2  3  5  6  7  8  9\n",
        "embedding_example(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
              "array([-0.38147095, -0.59538394,  0.65744424,  0.5225975 ,  0.07095776],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYxL9jm9ZnPk",
        "outputId": "2feb9677-299b-4357-d4db-d5f562513338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# One hot equivalente = [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "#               indices: 0  1  2  3  5  6  7  8  9\n",
        "embedding_example(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
              "array([-0.46625623,  0.4935111 , -0.4437881 ,  0.26713082,  0.1052963 ],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdM6qNXaY-Do",
        "outputId": "921db1e1-d863-4f95-fb2e-fb81e632266f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "embedding_example(np.array([0, 1, 2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=float32, numpy=\n",
              "array([[-0.37037277, -0.18537672,  0.18003024,  0.13673331, -2.0339165 ,\n",
              "         0.25801376, -0.69475514, -0.6896385 ,  0.37915477, -1.2440747 ],\n",
              "       [-0.39282772,  0.07648234, -0.48073792, -2.2546585 ,  0.04022843,\n",
              "         1.5837353 ,  0.09144673,  1.1881027 , -1.056005  ,  1.1102625 ],\n",
              "       [-1.7462479 ,  0.81345886, -0.11882789,  0.83243835,  0.11795305,\n",
              "        -0.6272579 , -0.7170191 , -0.00369342, -1.2439246 , -0.47591078]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38fqRUn9Rsdx"
      },
      "source": [
        "### Modelagem de redes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewZCj6PTZVAz"
      },
      "source": [
        "def create_features(tweets, max_len=50):\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(tweets)\n",
        "  sequences = tokenizer.texts_to_sequences(tweets)\n",
        "\n",
        "  tweet_pad = pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n",
        "\n",
        "  return tweet_pad, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRd4EKsiLc1M"
      },
      "source": [
        "tweet_pad, tokenizer = create_features(df['text'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweet_pad, df['target'].values, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49jlUkD8dHTa",
        "outputId": "f47b5366-15d0-42b2-ab5d-7037fa4332fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "tweet_pad[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([213, 730, 731, 570, 913,  45,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3C-iQwgdgNC",
        "outputId": "0f5374db-3f75-4438-fe29-114006afa17e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.index_word.get(213)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'always'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBX78PnsV9gc"
      },
      "source": [
        "def create_embedding_layer(embedding, tokenizer, embedding_dim, max_len=50, trainable=True):\n",
        "  words_ids = tokenizer.word_index\n",
        "\n",
        "  num_words = len(words_ids) + 1\n",
        "  embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "  for word, i in words_ids.items():\n",
        "    if i < num_words:\n",
        "      embedding_vector = embedding.get_vector(word)\n",
        "\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    embedding_layer = Embedding(num_words, embedding_dim, \n",
        "                                embeddings_initializer = Constant(embedding_matrix),\n",
        "                                input_length = max_len, trainable = trainable)\n",
        "\n",
        "    return embedding_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNzp_x29aaHk"
      },
      "source": [
        "def mount_lstm(embedding_layer):\n",
        "  \n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(embedding_layer)\n",
        "  model.add(SpatialDropout1D(0.3))\n",
        "  model.add(LSTM(40, dropout=0.3, recurrent_dropout=0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  optimzer = Adam(learning_rate=8e-5)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimzer, metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "def evaluate_neural_net(model, X_test, y_test, threshold = 0.5):\n",
        "\n",
        "  y_hat = model.predict(X_test)\n",
        "\n",
        "  y_hat_bool = y_hat > threshold\n",
        "\n",
        "  print(classification_report(y_test, y_hat_bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38rj6aHDR_6x"
      },
      "source": [
        "### Treinamento com diferentes word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gMRNAT0tCWt"
      },
      "source": [
        "Datasets e word2vec disponíveis: https://github.com/RaRe-Technologies/gensim-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii89W-t6lzm7"
      },
      "source": [
        "#### Word2Vec de notiícias do Google"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G22alrTAeyQR",
        "outputId": "8ae6cf79-8c63-450f-c10d-324d3daf23b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "google_300d_embedding = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 99.1% 1648.0/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pa6CI6Wc1M_",
        "outputId": "40c549dc-244b-4b2e-c123-4efe49b955c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "google_300d_layer = create_embedding_layer(google_300d_embedding, tokenizer, embedding_dim=300, max_len=50, trainable=False)\n",
        "google_lstm = mount_lstm(google_300d_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOCOpl5wgbeB",
        "outputId": "a270a17a-b9f5-4d9d-c1c0-2ceb3e8b5692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "google_lstm.fit(X_train, y_train, epochs=10, validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "127/127 [==============================] - 11s 87ms/step - loss: 0.6914 - accuracy: 0.5595 - val_loss: 0.6887 - val_accuracy: 0.5679\n",
            "Epoch 2/10\n",
            "127/127 [==============================] - 11s 85ms/step - loss: 0.6860 - accuracy: 0.5677 - val_loss: 0.6839 - val_accuracy: 0.5679\n",
            "Epoch 3/10\n",
            "127/127 [==============================] - 11s 84ms/step - loss: 0.6844 - accuracy: 0.5677 - val_loss: 0.6838 - val_accuracy: 0.5679\n",
            "Epoch 4/10\n",
            "127/127 [==============================] - 11s 85ms/step - loss: 0.6840 - accuracy: 0.5677 - val_loss: 0.6838 - val_accuracy: 0.5679\n",
            "Epoch 5/10\n",
            "127/127 [==============================] - 11s 85ms/step - loss: 0.6840 - accuracy: 0.5677 - val_loss: 0.6837 - val_accuracy: 0.5679\n",
            "Epoch 6/10\n",
            "127/127 [==============================] - 11s 85ms/step - loss: 0.6840 - accuracy: 0.5677 - val_loss: 0.6836 - val_accuracy: 0.5679\n",
            "Epoch 7/10\n",
            "127/127 [==============================] - 11s 85ms/step - loss: 0.6839 - accuracy: 0.5677 - val_loss: 0.6835 - val_accuracy: 0.5679\n",
            "Epoch 8/10\n",
            "127/127 [==============================] - 11s 85ms/step - loss: 0.6837 - accuracy: 0.5677 - val_loss: 0.6833 - val_accuracy: 0.5679\n",
            "Epoch 9/10\n",
            "127/127 [==============================] - 11s 85ms/step - loss: 0.6831 - accuracy: 0.5677 - val_loss: 0.6832 - val_accuracy: 0.5679\n",
            "Epoch 10/10\n",
            "127/127 [==============================] - 11s 83ms/step - loss: 0.6831 - accuracy: 0.5677 - val_loss: 0.6825 - val_accuracy: 0.5679\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd182f5dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAFt1CejesN8",
        "outputId": "ccc50722-97a0-4f04-dcc4-359dfa6ffb43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "evaluate_neural_net(google_lstm, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      1.00      0.72       577\n",
            "           1       0.00      0.00      0.00       439\n",
            "\n",
            "    accuracy                           0.57      1016\n",
            "   macro avg       0.28      0.50      0.36      1016\n",
            "weighted avg       0.32      0.57      0.41      1016\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl2NHaDwohPu",
        "outputId": "1d9042fb-af73-45aa-f929-e8be1c607860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "google_300d_layer_trainable = create_embedding_layer(google_300d_embedding, tokenizer, embedding_dim=300, max_len=50, trainable=True)\n",
        "google_lstm_trainable = mount_lstm(google_300d_layer_trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxyf8j70ooJP",
        "outputId": "1ef9f8ad-aa71-4fcb-d8aa-d70d66d87a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "google_lstm_trainable.fit(X_train, y_train, epochs=10, validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "127/127 [==============================] - 16s 127ms/step - loss: 0.6870 - accuracy: 0.5677 - val_loss: 0.6839 - val_accuracy: 0.5679\n",
            "Epoch 2/10\n",
            "127/127 [==============================] - 16s 123ms/step - loss: 0.6841 - accuracy: 0.5677 - val_loss: 0.6838 - val_accuracy: 0.5679\n",
            "Epoch 3/10\n",
            "127/127 [==============================] - 15s 122ms/step - loss: 0.6838 - accuracy: 0.5677 - val_loss: 0.6835 - val_accuracy: 0.5679\n",
            "Epoch 4/10\n",
            "127/127 [==============================] - 15s 122ms/step - loss: 0.6772 - accuracy: 0.5677 - val_loss: 0.6344 - val_accuracy: 0.5679\n",
            "Epoch 5/10\n",
            "127/127 [==============================] - 16s 122ms/step - loss: 0.4444 - accuracy: 0.8314 - val_loss: 0.5429 - val_accuracy: 0.7510\n",
            "Epoch 6/10\n",
            "127/127 [==============================] - 15s 122ms/step - loss: 0.3159 - accuracy: 0.8994 - val_loss: 0.6210 - val_accuracy: 0.7195\n",
            "Epoch 7/10\n",
            "127/127 [==============================] - 16s 124ms/step - loss: 0.2474 - accuracy: 0.9237 - val_loss: 0.5903 - val_accuracy: 0.7530\n",
            "Epoch 8/10\n",
            "127/127 [==============================] - 15s 121ms/step - loss: 0.1957 - accuracy: 0.9446 - val_loss: 0.5693 - val_accuracy: 0.7815\n",
            "Epoch 9/10\n",
            "127/127 [==============================] - 15s 122ms/step - loss: 0.1609 - accuracy: 0.9584 - val_loss: 0.6241 - val_accuracy: 0.7756\n",
            "Epoch 10/10\n",
            "127/127 [==============================] - 15s 121ms/step - loss: 0.1302 - accuracy: 0.9665 - val_loss: 0.6587 - val_accuracy: 0.7697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd054f3860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXmioLlOp0vU",
        "outputId": "7dd6597b-ccdf-4eb6-bcda-7310f4abc629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "evaluate_neural_net(google_lstm_trainable, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.77      0.79       577\n",
            "           1       0.72      0.77      0.74       439\n",
            "\n",
            "    accuracy                           0.77      1016\n",
            "   macro avg       0.77      0.77      0.77      1016\n",
            "weighted avg       0.77      0.77      0.77      1016\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHWWxQrimCVs"
      },
      "source": [
        "#### Word2Vec do Twitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeDHdKEKauqZ",
        "outputId": "83eb2f5c-e2b2-4d9d-ea31-099b0ddf1b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "twitter_embedding = KeyedVectors.load_word2vec_format('/content/drive/Shared drives/Grupo Turing/P&D/Áreas de Foco/NLP/Materiais/Aulas 2020/Aula Transfer Learning Word2Vec/word_embeddings/glove.twitter.27B.100d.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtHxcTifZBxF",
        "outputId": "5132dcdb-284d-4074-f1f9-572264608501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "twitter_layer = create_embedding_layer(twitter_embedding, tokenizer, embedding_dim=100, trainable = False)\n",
        "twitter_lstm = mount_lstm(twitter_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WittYXtV1St",
        "outputId": "a33ee575-458b-4d49-db88-221b592e0a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "twitter_lstm.fit(X_train, y_train, epochs=7, validation_data = (X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "127/127 [==============================] - 11s 85ms/step - loss: 0.6906 - accuracy: 0.5677 - val_loss: 0.6874 - val_accuracy: 0.5679\n",
            "Epoch 2/7\n",
            "127/127 [==============================] - 11s 84ms/step - loss: 0.6855 - accuracy: 0.5677 - val_loss: 0.6839 - val_accuracy: 0.5679\n",
            "Epoch 3/7\n",
            "127/127 [==============================] - 10s 82ms/step - loss: 0.6841 - accuracy: 0.5677 - val_loss: 0.6838 - val_accuracy: 0.5679\n",
            "Epoch 4/7\n",
            "127/127 [==============================] - 10s 82ms/step - loss: 0.6840 - accuracy: 0.5677 - val_loss: 0.6837 - val_accuracy: 0.5679\n",
            "Epoch 5/7\n",
            "127/127 [==============================] - 10s 81ms/step - loss: 0.6839 - accuracy: 0.5677 - val_loss: 0.6836 - val_accuracy: 0.5679\n",
            "Epoch 6/7\n",
            "127/127 [==============================] - 10s 82ms/step - loss: 0.6840 - accuracy: 0.5677 - val_loss: 0.6835 - val_accuracy: 0.5679\n",
            "Epoch 7/7\n",
            "127/127 [==============================] - 10s 82ms/step - loss: 0.6839 - accuracy: 0.5677 - val_loss: 0.6833 - val_accuracy: 0.5679\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd026bcf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGSKn69uY8KO",
        "outputId": "dd41b165-7083-4c6d-d1a0-da529b9b20d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "evaluate_neural_net(twitter_lstm, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      1.00      0.72       577\n",
            "           1       0.00      0.00      0.00       439\n",
            "\n",
            "    accuracy                           0.57      1016\n",
            "   macro avg       0.28      0.50      0.36      1016\n",
            "weighted avg       0.32      0.57      0.41      1016\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuQFgal7qHwb",
        "outputId": "6b36b18d-a689-43bf-f6d2-b66a90d9f54c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "twitter_layer_trainable = create_embedding_layer(twitter_embedding, tokenizer, embedding_dim=100, trainable = True)\n",
        "twitter_lstm_trainable = mount_lstm(twitter_layer_trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jKRxlTfrEaQ",
        "outputId": "c3743b94-6eea-458a-899f-8b797f46f670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "twitter_lstm_trainable.fit(X_train, y_train, epochs=7, validation_data = (X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.6895 - accuracy: 0.5679 - val_loss: 0.6859 - val_accuracy: 0.5679\n",
            "Epoch 2/7\n",
            "127/127 [==============================] - 13s 100ms/step - loss: 0.6843 - accuracy: 0.5677 - val_loss: 0.6838 - val_accuracy: 0.5679\n",
            "Epoch 3/7\n",
            "127/127 [==============================] - 13s 100ms/step - loss: 0.6840 - accuracy: 0.5677 - val_loss: 0.6839 - val_accuracy: 0.5679\n",
            "Epoch 4/7\n",
            "127/127 [==============================] - 13s 101ms/step - loss: 0.6840 - accuracy: 0.5677 - val_loss: 0.6837 - val_accuracy: 0.5679\n",
            "Epoch 5/7\n",
            "127/127 [==============================] - 13s 101ms/step - loss: 0.6831 - accuracy: 0.5677 - val_loss: 0.6826 - val_accuracy: 0.5679\n",
            "Epoch 6/7\n",
            "127/127 [==============================] - 13s 102ms/step - loss: 0.5893 - accuracy: 0.6713 - val_loss: 0.5533 - val_accuracy: 0.7569\n",
            "Epoch 7/7\n",
            "127/127 [==============================] - 13s 101ms/step - loss: 0.3734 - accuracy: 0.8723 - val_loss: 0.5325 - val_accuracy: 0.7785\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd00fc65c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9rBx-wSrJz2",
        "outputId": "24621eb5-d966-417a-951e-e3b0977f9b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "evaluate_neural_net(twitter_lstm_trainable, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.82      0.81       577\n",
            "           1       0.75      0.73      0.74       439\n",
            "\n",
            "    accuracy                           0.78      1016\n",
            "   macro avg       0.77      0.77      0.77      1016\n",
            "weighted avg       0.78      0.78      0.78      1016\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVO_TvAs3sRb"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_noMfhQ3qxO",
        "outputId": "462027a1-1149-49e2-a299-234f00ab0a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 23.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 38.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=a533130650337bb85351e08a088000d3d9ac276c7c127a1782eac9d9ba6a3145\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfMw4rWt4BYU"
      },
      "source": [
        "from transformers import AutoTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
        "bert = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyD4sYy34D_i"
      },
      "source": [
        "def bert_encode(tweets, Tokenizer, maximum_len):\n",
        "  \"\"\"\n",
        "  Tokeniza, converte texto para id do vocabulário, constroi sequência e cria vetor com máscara de atenção.\n",
        "  \"\"\"\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  \n",
        "  for tweet in tweets:\n",
        "      encoded = Tokenizer.encode_plus(tweet,\n",
        "                                      add_special_tokens=True,\n",
        "                                      truncation=True,\n",
        "                                      max_length=maximum_len,\n",
        "                                      pad_to_max_length=True,\n",
        "                                      return_attention_mask=True)\n",
        "      \n",
        "      input_ids.append(encoded['input_ids'])\n",
        "      attention_masks.append(encoded['attention_mask'])\n",
        "        \n",
        "  return np.array(input_ids), np.array(attention_masks)\n",
        "\n",
        "def mount_bert_input(tweets, targets, Tokenizer, maximum_len=128, test_size=0.3):\n",
        "  features = bert_encode(tweets, Tokenizer, maximum_len)\n",
        "\n",
        "  return features, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIMkyzx44HP8",
        "outputId": "402d8aab-86c1-4e8c-dd29-5c4aea94aae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_bert['text'], df_bert['target'], test_size = 0.10, random_state=42)\n",
        "\n",
        "X_train, y_train = mount_bert_input(X_train, y_train, bert_tokenizer)\n",
        "X_test, y_test = mount_bert_input(X_test, y_test, bert_tokenizer);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gclXHVV4I6J"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "bert.compile(optimizer=optimizer, loss='binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTgiRHnJ4LNT",
        "outputId": "9a1457cd-da52-481c-ed3c-20e13a9af190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "bert.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  109482240 \n",
            "_________________________________________________________________\n",
            "dropout_75 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "=================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-zzu9LKCAtr",
        "outputId": "2a81d393-b006-4a86-c7eb-040584d35574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "bert.fit(X_train, y_train, epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "215/215 [==============================] - 102s 476ms/step - loss: 0.5743\n",
            "Epoch 2/3\n",
            "215/215 [==============================] - 102s 477ms/step - loss: 0.5158\n",
            "Epoch 3/3\n",
            "215/215 [==============================] - 102s 476ms/step - loss: 0.4886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa60f6fd710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdzZV4gjCJMZ"
      },
      "source": [
        "y_hat = bert.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNOfyS3BPaRf"
      },
      "source": [
        "y_hat_bool = []\n",
        "for i in y_hat[0]:\n",
        "  y_hat_bool.append(i[0] > 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j4wRlGqPryE",
        "outputId": "078a38b2-9d5c-4c3e-ff81-0487cc1aebe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_hat_bool, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.91      0.81      0.86       479\n",
            "        True       0.73      0.87      0.79       283\n",
            "\n",
            "    accuracy                           0.83       762\n",
            "   macro avg       0.82      0.84      0.82       762\n",
            "weighted avg       0.84      0.83      0.83       762\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIaX1faTRdgb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}