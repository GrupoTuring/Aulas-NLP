{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Skip-gram\n",
    "Neste notebook, estaremos treinando uma das variantes do Word2Vec, o Skip-Gram. Para isso, vamos construir uma rede neural com apenas uma hidden layer. Sendo assim, a nossa arquitetura vai operar como um modelo de linguagem, ou seja, a tarefa que a rede irá realizar é \"dada uma palavra *X*, tente prever a próxima palavra *Y*\". \n",
    "Porém, para extrair os nossos word vectors, usaremos apenas os pesos da nossa camada escondida. Assim, o nosso objetivo final será aprender os pesos da nossa hidden layer. \n",
    "\n",
    "Nesse cenário, cada um dos elementos do nosso word vector funcionará como uma \"feature\" da nossa palavra representada, ou seja, cada uma dessas dimensões é responsável por uma característica da composição semântica dessa palavra. Como estamos mapeando os seus significados a partir do contexto em que a palavra está inserida, a ideia principal que o word2vec utiliza, é a de que palavras que acontecem em *contextos* semelhantes também possuem *significados* semelhantes. \n",
    "\n",
    "#### Importando bibliotecas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo o nosso arquivo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"biblia-em-txt.txt\", \"r\")\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BÍBLIA SAGRADA\\nTradução: João Ferreira de Almeida\\nEdição Revista e Corrigida\\n \\t\\nANTIGO TESTAMENTO\\n\\nGÊNESIS\\n GÊNESIS 1\\n1 No princípio criou Deus os céus e a terra.\\n2 A terra era sem forma e vazia; e havia trevas sobre a face do abismo, mas o Espírito de Deus pairava sobre a face das águas.\\n3 Disse Deus: haja luz. E houve luz.\\n4 Viu Deus que a luz era boa; e fez separação entre a luz e as trevas.\\n5 E Deus chamou à luz dia, e às trevas noite. E foi a tarde e a manhã, o dia primeiro.\\n6 E disse Deus:'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizando o texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentece(sent):\n",
    "    tokenized = word_tokenize(sent)\n",
    "    tokenized = [token.lower() for token in tokenized if token.isalpha()]\n",
    "    return tokenized\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_senteces = [tokenize_sentece(sent) for sent in sentences]\n",
    "    return tokenized_senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = tokenize_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bíblia',\n",
       "  'sagrada',\n",
       "  'tradução',\n",
       "  'joão',\n",
       "  'ferreira',\n",
       "  'de',\n",
       "  'almeida',\n",
       "  'edição',\n",
       "  'revista',\n",
       "  'e',\n",
       "  'corrigida',\n",
       "  'antigo',\n",
       "  'testamento',\n",
       "  'gênesis',\n",
       "  'gênesis',\n",
       "  'no',\n",
       "  'princípio',\n",
       "  'criou',\n",
       "  'deus',\n",
       "  'os',\n",
       "  'céus',\n",
       "  'e',\n",
       "  'a',\n",
       "  'terra'],\n",
       " ['a',\n",
       "  'terra',\n",
       "  'era',\n",
       "  'sem',\n",
       "  'forma',\n",
       "  'e',\n",
       "  'vazia',\n",
       "  'e',\n",
       "  'havia',\n",
       "  'trevas',\n",
       "  'sobre',\n",
       "  'a',\n",
       "  'face',\n",
       "  'do',\n",
       "  'abismo',\n",
       "  'mas',\n",
       "  'o',\n",
       "  'espírito',\n",
       "  'de',\n",
       "  'deus',\n",
       "  'pairava',\n",
       "  'sobre',\n",
       "  'a',\n",
       "  'face',\n",
       "  'das',\n",
       "  'águas']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapeando as palavras do nosso vocabulário\n",
    "Antes de partirmos para a arquitetura da rede em si, precisamos preparar o nosso corpus. \n",
    "\n",
    "A primeira parte é construir um dicionário com todas as palavras do nosso texto - `vocab` - e os seus índices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(sent_tokenized):\n",
    "    i = 0\n",
    "    vocab = {}\n",
    "    for sent in sent_tokenized:\n",
    "        for token in sent:\n",
    "            if token in vocab:\n",
    "                continue\n",
    "            else:\n",
    "                vocab[token] = i\n",
    "                i+=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = make_vocab(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24714\n"
     ]
    }
   ],
   "source": [
    "V = len(vocab)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, podemos observar que o nosso vocabulário possui 24714 palavras únicas. \n",
    "\n",
    "Agora, precisamos gerar as palavras centrais com as suas respectivas palavras de contexto. \n",
    "\n",
    "Para o modelo Skip-Gram, devemos montar uma tupla para a palavra central com cada uma das suas palavras de contexto. \n",
    "\n",
    "Observe o exemplo: \n",
    "![](http://mccormickml.com/assets/word2vec/training_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(words, C):\n",
    "    i = 0\n",
    "    while i < len(words): \n",
    "        center_word = words[i] \n",
    "        if i < C:\n",
    "            context_words = words[0:i] + words[(i+1):(i+C+1)]\n",
    "        else:\n",
    "            context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        \n",
    "        \n",
    "        for w in range(len(context_words)):\n",
    "            context_word = context_words[w]\n",
    "            yield center_word, context_word\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(sent_tokens, vocab, V, C):\n",
    "    '''\n",
    "    inputs:\n",
    "        sent_tokens: lista de listas tokenizadas \n",
    "        vocab -- dicionário com {word:index}\n",
    "        V -- tamanho do vocabulário\n",
    "        C -- tamanho da janela de contexto\n",
    "    \n",
    "    outputs: tupla (x,y), em que x=palavra central e y=palavra de contexto\n",
    "    '''\n",
    "    \n",
    "    windows_and_target = []\n",
    "    for sentence in sent_tokens:\n",
    "        for x, y in get_windows(sentence,C):\n",
    "            windows_and_target.append((x,y))\n",
    "        \n",
    "    return windows_and_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No princípio criou Deus os céus e a terra.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplo = text[120:162]\n",
    "exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('no', 'princípio'),\n",
       " ('no', 'criou'),\n",
       " ('princípio', 'no'),\n",
       " ('princípio', 'criou'),\n",
       " ('princípio', 'deus'),\n",
       " ('criou', 'no'),\n",
       " ('criou', 'princípio'),\n",
       " ('criou', 'deus'),\n",
       " ('criou', 'os'),\n",
       " ('deus', 'princípio'),\n",
       " ('deus', 'criou'),\n",
       " ('deus', 'os'),\n",
       " ('deus', 'céus'),\n",
       " ('os', 'criou'),\n",
       " ('os', 'deus'),\n",
       " ('os', 'céus'),\n",
       " ('os', 'e'),\n",
       " ('céus', 'deus'),\n",
       " ('céus', 'os'),\n",
       " ('céus', 'e'),\n",
       " ('céus', 'a'),\n",
       " ('e', 'os'),\n",
       " ('e', 'céus'),\n",
       " ('e', 'a'),\n",
       " ('e', 'terra'),\n",
       " ('a', 'céus'),\n",
       " ('a', 'e'),\n",
       " ('a', 'terra'),\n",
       " ('terra', 'e'),\n",
       " ('terra', 'a')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = tokenize_text(exemplo)\n",
    "data_exemplo = make_dataset(sent, vocab, V, C=2)\n",
    "data_exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_dataset(sent_tokens, vocab, V, C=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação com Numpy\n",
    "**Rede Neural para o modelo Skip-Gram:**\n",
    "\n",
    "![](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "\n",
    "\n",
    "\n",
    "A rede pode ser descrita pelas seguintes equações: \n",
    "<br>\n",
    "$$Z_1 = W_1X + b_1$$\n",
    "$$Z_2 = W_1h + b_2$$\n",
    "$$ŷ = softmax(Z_2)$$\n",
    "<br>\n",
    "\n",
    "Em que a função softmax pode ser definida:\n",
    "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_{k=1}^N e^{x_k}}$$\n",
    "\n",
    "Nossa função de custo é a entropia cruzada, que é definida da seguinte forma:\n",
    "$$J = - \\sum_{k=1}^V y_k \\ln\\hat{y}_k$$\n",
    "Em que V denota o número de palavras no vocabulário. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_one_hot_vector(word, vocab, V):\n",
    "    '''\n",
    "    inputs:\n",
    "    word -- string\n",
    "    vocab -- dicionário com {word:index}\n",
    "    V -- tamanho do vocabulário\n",
    "    \n",
    "    output: vetor one-hot da palavra \n",
    "    '''\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[vocab[word]] = 1\n",
    "    \n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(raw_batch, vocab, V, batch_size):\n",
    "    '''\n",
    "    inputs:\n",
    "        raw_batch --  lista de tuplas (x,y)\n",
    "        vocab -- dicionário com {word:index}\n",
    "        V -- tamanho do vocabulário\n",
    "        batch_size -- tamanho da batch   \n",
    "    '''\n",
    "    \n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for x, y in raw_batch:\n",
    "        while len(batch_x) < batch_size:\n",
    "            batch_x.append(word_to_one_hot_vector(x, vocab, V))\n",
    "            batch_y.append(word_to_one_hot_vector(y, vocab, V))\n",
    "        else:\n",
    "            return np.array(batch_x).T, np.array(batch_y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(V,emb_size):\n",
    "    '''\n",
    "    inputs:\n",
    "        V -- tamanho do vocabulário\n",
    "        emb_size -- número de dimensões do embedding \n",
    "    outputs: W1, W2, b1, b2\n",
    "    '''\n",
    "    W1 = np.random.rand(emb_size,V)\n",
    "    W2 = np.random.rand(V,emb_size)\n",
    "    b1 = np.random.rand(emb_size,1)\n",
    "    b2 = np.random.rand(V,1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z, axis=0) \n",
    "    soft = e_z / sum_e_z\n",
    "    return soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, W1, W2):\n",
    "    z1 = W1@x \n",
    "    h = z1\n",
    "    z2 = W2@z1 \n",
    "    yhat = softmax(z2)\n",
    "    return yhat, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, yhat, batch_size):\n",
    "    logprobs = y * np.log(yhat)   \n",
    "    cost = - (1/batch_size) * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, batch_size):\n",
    "    grad_W1 = (1/batch_size) * np.dot(np.dot(W2.T,(yhat - y)), x.T)\n",
    "    grad_W2 = (1/batch_size) * np.dot((yhat - y), h.T)\n",
    "    return grad_W1, grad_W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0| epoch 1| cost: 15.63539376762904\n",
      "iteration: 200| epoch 1| cost: 0.11261365110622698\n",
      "iteration: 400| epoch 1| cost: 0.03005449474070234\n",
      "iteration: 600| epoch 1| cost: 0.017110867542549264\n",
      "iteration: 800| epoch 1| cost: 0.011929401257448558\n",
      "iteration: 1000| epoch 1| cost: 0.00914774719287372\n",
      "iteration: 1200| epoch 1| cost: 0.007414469894138136\n",
      "iteration: 1400| epoch 1| cost: 0.006231658699166231\n",
      "iteration: 1600| epoch 1| cost: 0.005373353877290556\n",
      "iteration: 1800| epoch 1| cost: 0.004722285082881849\n",
      "iteration: 2000| epoch 1| cost: 0.004211570077451198\n",
      "iteration: 2200| epoch 1| cost: 0.0038002906637602182\n",
      "iteration: 2400| epoch 1| cost: 0.0034620143313829287\n",
      "iteration: 2600| epoch 1| cost: 0.003178908330699638\n",
      "iteration: 2800| epoch 1| cost: 0.0029385069688249075\n",
      "iteration: 3000| epoch 1| cost: 0.0027318346734249792\n",
      "iteration: 3200| epoch 1| cost: 0.002552264975060238\n",
      "iteration: 3400| epoch 1| cost: 0.002394799509667702\n",
      "iteration: 3600| epoch 1| cost: 0.0022555971308617505\n",
      "iteration: 3800| epoch 1| cost: 0.002131657466720702\n",
      "iteration: 4000| epoch 1| cost: 0.0020206029001327073\n",
      "iteration: 4200| epoch 1| cost: 0.0019205250258607215\n",
      "iteration: 4400| epoch 1| cost: 0.0018298743884743974\n",
      "iteration: 4600| epoch 1| cost: 0.0017473799088921018\n",
      "iteration: 4800| epoch 1| cost: 0.0016719890766469505\n",
      "iteration: 5000| epoch 1| cost: 0.0016028229219937759\n",
      "iteration: 5200| epoch 1| cost: 0.0015391416762933381\n",
      "iteration: 5400| epoch 1| cost: 0.00148031827403415\n",
      "iteration: 5600| epoch 1| cost: 0.001425817684273348\n",
      "iteration: 5800| epoch 1| cost: 0.0013751806285980661\n",
      "iteration: 6000| epoch 1| cost: 0.0013280106366685346\n",
      "iteration: 6200| epoch 1| cost: 0.0012839636678546721\n",
      "iteration: 6400| epoch 1| cost: 0.0012427397243974657\n",
      "iteration: 6600| epoch 1| cost: 0.0012040760241874982\n",
      "iteration: 6800| epoch 1| cost: 0.0011677414052351644\n",
      "iteration: 7000| epoch 1| cost: 0.001133531709775527\n",
      "iteration: 7200| epoch 1| cost: 0.001101265953976052\n",
      "iteration: 7400| epoch 1| cost: 0.0010707831316479639\n",
      "iteration: 7600| epoch 1| cost: 0.0010419395322565556\n",
      "iteration: 7800| epoch 1| cost: 0.001014606480029889\n",
      "iteration: 8000| epoch 1| cost: 0.0009886684184182897\n",
      "iteration: 8200| epoch 1| cost: 0.0009640212800180021\n",
      "iteration: 8400| epoch 1| cost: 0.0009405710938559742\n",
      "iteration: 8600| epoch 1| cost: 0.0009182327900710067\n",
      "iteration: 8800| epoch 1| cost: 0.0008969291708018353\n",
      "iteration: 9000| epoch 1| cost: 0.000876590020402053\n",
      "iteration: 9200| epoch 1| cost: 0.0008571513336632195\n",
      "iteration: 9400| epoch 1| cost: 0.0008385546442713867\n",
      "iteration: 9600| epoch 1| cost: 0.0008207464384210098\n",
      "iteration: 9800| epoch 1| cost: 0.0008036776414609182\n",
      "iteration: 10000| epoch 1| cost: 0.0007873031670552964\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 1\n",
    "ALPHA = 0.001\n",
    "emb_size = 300\n",
    "\n",
    "\n",
    "W1, W2, b1, b2 = initialize_params(V,emb_size)\n",
    "for i in range(EPOCHS):\n",
    "    for j in range(int(len(dataset)/BATCH_SIZE)):\n",
    "        x,y = get_batch(dataset, vocab, V, BATCH_SIZE)\n",
    "        yhat, h = forward(x, W1, W2)\n",
    "        cost = compute_cost(y, yhat, BATCH_SIZE)\n",
    "        if j%200 == 0:\n",
    "            print(f\"iteration: {j}| epoch {i+1}| cost: {cost}\")\n",
    "        grad_W1, grad_W2 = back_prop(x, yhat, y, h, W1, W2, BATCH_SIZE)\n",
    "        W1 -= ALPHA*grad_W1\n",
    "        W2-= ALPHA*grad_W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = (W1.T + W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_embeddings = {}\n",
    "for word, i in vocab.items():\n",
    "    encoded_embeddings[word] = list(embeddings[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'bible_embeddings_skipgram_300.json', 'w') as file:\n",
    "    file.write(json.dumps(encoded_embeddings)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando os embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('bible_embeddings_skipgram_300.json') as json_file:\n",
    "    emb = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x,y):\n",
    "    return np.dot(x.T,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "\n",
    "def find_most_similar(x, word_emb):\n",
    "    similar = {}\n",
    "    for word, vec in word_emb.items():\n",
    "        sim = cosine_similarity(np.array(word_emb[x]),np.array(vec))\n",
    "        if word != x:\n",
    "            similar[word] = sim\n",
    "    similar =  {k: v for k, v in sorted(similar.items(), key=lambda item: item[1], reverse=True)}\n",
    "    most_10_similar = list(similar.keys())[:10]\n",
    "    most_similar = [(x, similar[x]) for x in most_10_similar]\n",
    "    return most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda57861a77cbd74bdab3b3f15ccf4f3179"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
